from fastapi import FastAPI, Request
import requests

# âœ… Create FastAPI app instance before using it
app = FastAPI()

# ğŸ” URL of the local LLM model runner (adjust if needed)
LLM_URL = "http://local-llm:11434/api/generate"

@app.post("/chat")
async def chat(req: Request):
    """
    Accepts a POST request with a JSON payload containing a 'prompt',
    forwards it to the local LLM endpoint, and returns the generated response.
    """
    data = await req.json()
    prompt = data.get("prompt")
    
    # ğŸ“¨ Send request to local model
    response = requests.post(LLM_URL, json={"prompt": prompt, "stream": False})
    
    # ğŸ§¾ Return model's response as JSON
    return response.json()
