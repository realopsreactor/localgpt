
## ğŸ“Œ Project Name: `LocalGPT Assistant â€“ Docker Model Runner Edition`

### ğŸ‘¥ Team Context

You're part of the "LLM Enablement" team within a developer tools org. Your squad is exploring new ways to empower AI/ML developers to build locally-hosted applications without depending on cloud APIs. You're assigned to evaluate **Docker Model Runner** and demonstrate its practical integration into an existing multi-service AI app stack.

---

## ğŸ¯ Mission Objective

> **Integrate Docker Model Runner as the local LLM backend for an existing ChatGPT-style app (FastAPI + Gradio), replacing external API calls.**

This will allow:

* Full local inference using open-source LLMs (e.g. SmolLM, TinyLlama, Gemma)

* Reproducible deployment using Docker Compose

* Offline or air-gapped development

---

## ğŸ§± Provided Project Structure

You are given the following repository:

```
localgpt/
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI backend for prompt handling
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ ui/
â”‚   â”œâ”€â”€ app.py               # Gradio frontend
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â””â”€â”€ docker-compose.yaml      # [To be created by YOU]
```

---

## ğŸ§© Your Deliverables

### ğŸ” Phase 1: Learn and Explore Docker Model Runner

Before integrating, understand how Docker Model Runner works:

1. âœ… Enable it in Docker Desktop:

   * Settings > Features in Development > Enable Docker Model Runner

   * Restart Docker Desktop

2. âœ… Try out basic commands:

â €
```
docker model pull ai/smollm2
docker model run ai/smollm2 "How do you work?"
```

ğŸ‘‰ Observe how it pulls, loads, and responds with no external API involved.

---

### ğŸ› ï¸ Phase 2: Modify FastAPI to Use Local Model

Update `app/main.py` to interact with the Model Runner's OpenAI-compatible endpoint:

```
LLM_URL = "http://model-runner.docker.internal/engines/llama.cpp/v1/chat/completions"

@app.post("/chat")
async def chat(req: Request):
    data = await req.json()
    prompt = data.get("prompt")
    payload = {
        "model": "ai/smollm2",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ]
    }
    response = requests.post(LLM_URL, json=payload)
    return response.json()
```

âœ… This allows FastAPI to relay prompts to the local LLM using a standard OpenAI-style API.

---

### âš™ï¸ Phase 3: Write Docker Compose Spec

Create a `docker-compose.yaml` that:

* Starts the Docker Model Runner model provider

* Boots the FastAPI and UI containers in correct sequence

* Automatically injects model metadata to services

```
version: "3"

services:
  model:
    provider:
      type: model
      options:
        model: ai/smollm2

  fastapi:
    build:
      context: ./app
    ports:
      - "8000:8000"
    depends_on:
      - model

  ui:
    build:
      context: ./ui
    ports:
      - "8501:8501"
    depends_on:
      - fastapi
```

ğŸ“Œ **Note**: No changes are needed in the UI â€” it communicates with the FastAPI backend as before.

---

## ğŸ§ª How to Run the Stack

```
docker compose up --build
```

Visit:

* [http://localhost:8501](http://localhost:8501/) â€“ UI

* [http://localhost:8000/chat](http://localhost:8000/chat) â€“ API

---

## ğŸ§  Learning Goals

By the end of this project, youâ€™ll:

âœ… Understand **how Docker Model Runner manages and runs LLMs locally**  
âœ… Replace hosted LLM APIs with **local inference endpoints**  
âœ… Learn how to **package model providers in Docker Compose**  
âœ… Build confidence in **open-source model deployment workflows**  

